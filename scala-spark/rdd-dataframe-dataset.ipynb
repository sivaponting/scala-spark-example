{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f1fd53",
   "metadata": {},
   "source": [
    "## Create Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0508d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Seq[String] = List(language, users-account)\n",
       "data: Seq[(String, Int)] = List((Java,20000), (Python,100000), (Scala,3000))\n",
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[9] at parallelize at <console>:38\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Seq(\"language\", \"users-account\")\n",
    "val data = Seq((\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000))\n",
    "val rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d1898",
   "metadata": {},
   "source": [
    "## Convert Spark RDD to DataFrame\n",
    "\n",
    "Converting Spark RDD to DataFrame can be done using toDF(), createDataFrame() and transforming rdd[Row] to the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dafb23",
   "metadata": {},
   "source": [
    "## Convert RDD to DataFrame – Using toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c92ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: integer (nullable = false)\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_account: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfFromRDD1: org.apache.spark.sql.DataFrame = [_1: string, _2: int]\n",
       "dfFromRDD1_1: org.apache.spark.sql.DataFrame = [language: string, users_account: int]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()\n",
    "\n",
    "// toDF() has another signature that takes arguments to define column names as shown below.\n",
    "\n",
    "val dfFromRDD1_1 = rdd.toDF(\"language\", \"users_account\")\n",
    "dfFromRDD1_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae39217",
   "metadata": {},
   "source": [
    "## Convert RDD to DataFrame – Using createDataFrame()\n",
    "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument. and chain it with toDF() to specify names to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1aa3bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Seq[String] = List(language, users_account)\n",
       "dfFromRDD2: org.apache.spark.sql.DataFrame = [language: string, users_account: int]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Seq(\"language\", \"users_account\")\n",
    "val dfFromRDD2 = spark.createDataFrame(rdd).toDF(columns: _*)\n",
    "// Here, we are using scala operator :_*  to explode columns array to comma-separated values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c51052",
   "metadata": {},
   "source": [
    "## Using RDD Row type RDD[Row] to DataFrame\n",
    "\n",
    "Spark createDataFrame() has another signature which takes the RDD[Row] type and schema for column names as arguments. To use this first, we need to convert our “rdd” object from RDD[T] to RDD[Row]. To define a schema, we use StructType that takes an array of StructField. And StructField takes column name, data type and nullable/not as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e47b6733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.StructType\n",
       "import org.apache.spark.sql.types.StructField\n",
       "import org.apache.spark.sql.types.StringType\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(language,StringType,true), StructField(users_account,StringType,true))\n",
       "rowRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[11] at map at <console>:52\n",
       "dfFromRDD3: org.apache.spark.sql.DataFrame = [language: string, users_account: string]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.StructType\n",
    "import org.apache.spark.sql.types.StructField\n",
    "import org.apache.spark.sql.types.StringType\n",
    "\n",
    "//From RDD (USING createDataFrame and Adding schema using StructType)\n",
    "val schema = StructType(columns.map(fieldName => StructField(fieldName, StringType, nullable=true)))\n",
    "\n",
    "//convert RDD[T] to RDD[Row]\n",
    "val rowRDD = rdd.map(attributes => Row(attributes._1, attributes._2))\n",
    "\n",
    "val dfFromRDD3 = spark.createDataFrame(rowRDD, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfd95d",
   "metadata": {},
   "source": [
    "## Convert Spark RDD to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3212c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|_1    |_2    |\n",
      "+------+------+\n",
      "|Java  |20000 |\n",
      "|Python|100000|\n",
      "|Scala |3000  |\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.createDataset(rdd)\n",
    "ds.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35bf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
