{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b3d929",
   "metadata": {},
   "source": [
    "## Spark RDD reduce() function example\n",
    "RDD reduce() function takes function type as an argument and returns the RDD with the same type as input. It reduces the elements of the input RDD using the binary operator specified\n",
    "\n",
    "#### Syntax\n",
    "def reduce(f: (T, T) => T): T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4380b",
   "metadata": {},
   "source": [
    "## Reduce a list â€“ Calculate min, max, and total of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff9aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output min: 1\n",
      "output max: 5\n",
      "output sum: 20\n",
      "--------------------------\n",
      "output min: 1\n",
      "output max: 5\n",
      "output sum: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listRdd = spark.sparkContext.parallelize(List(1,2,3,4,5,3,2))\n",
    "println(\"output min: \"+ listRdd.reduce(_ min _))\n",
    "println(\"output max: \"+ listRdd.reduce(_ max _))\n",
    "println(\"output sum: \"+ listRdd.reduce(_ + _))\n",
    "\n",
    "println(\"--------------------------\")\n",
    "//Alternatively, you can also write the above operations as below\n",
    "println(\"output min: \"+ listRdd.reduce((a, b) => a min b))\n",
    "println(\"output max: \"+ listRdd.reduce((a, b) => a max b))\n",
    "println(\"output sum: \"+ listRdd.reduce((a, b) => a + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082ad43",
   "metadata": {},
   "source": [
    "## Reduce function on Tupple RDD(String,Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86dc58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output min: 1\n",
      "output max: 60\n",
      "output sum: 181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputRDD: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[10] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputRDD = spark.sparkContext.parallelize(List((\"Z\", 1), (\"A\", 20), (\"B\", 30), (\"C\", 40), (\"B\", 30), (\"B\", 60)))\n",
    "println(\"output min: \"+ inputRDD.reduce((a, b) => (\"min\", a._2 min b._2))._2)\n",
    "println(\"output max: \"+ inputRDD.reduce((a, b) => (\"max\", a._2 max b._2))._2)\n",
    "println(\"output sum: \"+ inputRDD.reduce((a, b) => (\"sum\", a._2 + b._2))._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582427c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
